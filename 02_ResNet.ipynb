{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb4f7f3c-166b-49f7-8d0e-8cd56b33226d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchinfo import summary\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "#from tqdm.notebook import tqdm\n",
    "\n",
    "from train_test import train, test\n",
    "\n",
    "# device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948e4808-f4c9-4aae-9077-9e01b545a0e3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f230b6f-d17c-42b6-88f2-a32c31ab9c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "844\n",
      "94\n",
      "157\n"
     ]
    }
   ],
   "source": [
    "VAL_SIZE = 0.1\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# download the training and testing datasets\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data',\n",
    "                                           train=True,\n",
    "                                           transform=transforms.ToTensor(),\n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='./root',\n",
    "                                          train=False,\n",
    "                                          transform=transforms.ToTensor(),\n",
    "                                          download=True)\n",
    "\n",
    "# split the training set into train and validation sets\n",
    "train_indices, val_indices, _, _ = train_test_split(range(len(train_dataset)),\n",
    "                                                    train_dataset.targets,\n",
    "                                                    stratify=train_dataset.targets,\n",
    "                                                    test_size=VAL_SIZE)\n",
    "\n",
    "train_split = Subset(train_dataset, train_indices)\n",
    "val_split = Subset(train_dataset, val_indices)\n",
    "\n",
    "# now create the batches of the train, val, test sets\n",
    "train_loader = DataLoader(train_split, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_split, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "print(len(train_loader))\n",
    "print(len(val_loader))\n",
    "print(len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98aa0671-5fb8-4f9a-9219-a46e8da47253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show some samples from test set\n",
    "\n",
    "samples = iter(test_loader)\n",
    "sample_data, sample_targets = next(samples)\n",
    "print((sample_data[0][0]).size())\n",
    "\n",
    "for i in range(6):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    plt.imshow(sample_data[i][0], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422e201f-0f18-477b-b294-128d1086a246",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9e5c501a-2c55-4d27-8c95-8d3ae9b87e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet style convolutional neural network\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    \n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=inplanes, out_channels=planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.conv1_bn = nn.BatchNorm2d(planes)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=planes, out_channels=planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.conv2_bn = nn.BatchNorm2d(planes)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        \n",
    "        out = self.relu(self.conv1_bn(self.conv1(x)))\n",
    "        out = self.conv2_bn(self.conv2(out)) # apply the relu after the skipped connection addition\n",
    "        \n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        \n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out\n",
    "        \n",
    "        \n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.inplanes = 8\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.conv1_bn = nn.BatchNorm2d(self.inplanes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=3, padding=1)\n",
    "        \n",
    "        self.layer1 = self._make_layer(block, 8, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 16, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 32, layers[2], stride=2)\n",
    "        \n",
    "        self.adaptAvgPool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(32, num_classes)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def _make_layer(self, block, planes, n_blocks, stride=1):\n",
    "        downsample = None\n",
    "        \n",
    "        if stride != 1 or self.inplanes != planes:\n",
    "            downsample = nn.Sequential(nn.Conv2d(in_channels=self.inplanes, out_channels=planes, kernel_size=1, stride=stride, bias=False),\n",
    "                                       nn.BatchNorm2d(planes))\n",
    "        \n",
    "        # the first convolutional layer for each block which reduce the size of the features space\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        \n",
    "        self.inplanes = planes\n",
    "        \n",
    "        # the other convolutional layer of the block in which the features dimension and the channels are the same for all the layers\n",
    "        for _ in range(1, n_blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1_bn(self.conv1(x)))\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        \n",
    "        x = self.adaptAvgPool(x)\n",
    "        x = x.reshape(x.shape[0], -1) # flatten the out in a single vector\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f7da505b-585f-4ee5-866c-66eae7838469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ResNet                                   [64, 10]                  --\n",
       "├─Conv2d: 1-1                            [64, 8, 14, 14]           392\n",
       "├─BatchNorm2d: 1-2                       [64, 8, 14, 14]           16\n",
       "├─ReLU: 1-3                              [64, 8, 14, 14]           --\n",
       "├─MaxPool2d: 1-4                         [64, 8, 5, 5]             --\n",
       "├─Sequential: 1-5                        [64, 8, 5, 5]             --\n",
       "│    └─BasicBlock: 2-1                   [64, 8, 5, 5]             --\n",
       "│    │    └─Conv2d: 3-1                  [64, 8, 5, 5]             576\n",
       "│    │    └─BatchNorm2d: 3-2             [64, 8, 5, 5]             16\n",
       "│    │    └─ReLU: 3-3                    [64, 8, 5, 5]             --\n",
       "│    │    └─Conv2d: 3-4                  [64, 8, 5, 5]             576\n",
       "│    │    └─BatchNorm2d: 3-5             [64, 8, 5, 5]             16\n",
       "│    │    └─ReLU: 3-6                    [64, 8, 5, 5]             --\n",
       "│    └─BasicBlock: 2-2                   [64, 8, 5, 5]             --\n",
       "│    │    └─Conv2d: 3-7                  [64, 8, 5, 5]             576\n",
       "│    │    └─BatchNorm2d: 3-8             [64, 8, 5, 5]             16\n",
       "│    │    └─ReLU: 3-9                    [64, 8, 5, 5]             --\n",
       "│    │    └─Conv2d: 3-10                 [64, 8, 5, 5]             576\n",
       "│    │    └─BatchNorm2d: 3-11            [64, 8, 5, 5]             16\n",
       "│    │    └─ReLU: 3-12                   [64, 8, 5, 5]             --\n",
       "├─Sequential: 1-6                        [64, 16, 3, 3]            --\n",
       "│    └─BasicBlock: 2-3                   [64, 16, 3, 3]            --\n",
       "│    │    └─Conv2d: 3-13                 [64, 16, 3, 3]            1,152\n",
       "│    │    └─BatchNorm2d: 3-14            [64, 16, 3, 3]            32\n",
       "│    │    └─ReLU: 3-15                   [64, 16, 3, 3]            --\n",
       "│    │    └─Conv2d: 3-16                 [64, 16, 3, 3]            2,304\n",
       "│    │    └─BatchNorm2d: 3-17            [64, 16, 3, 3]            32\n",
       "│    │    └─Sequential: 3-18             [64, 16, 3, 3]            160\n",
       "│    │    └─ReLU: 3-19                   [64, 16, 3, 3]            --\n",
       "│    └─BasicBlock: 2-4                   [64, 16, 3, 3]            --\n",
       "│    │    └─Conv2d: 3-20                 [64, 16, 3, 3]            2,304\n",
       "│    │    └─BatchNorm2d: 3-21            [64, 16, 3, 3]            32\n",
       "│    │    └─ReLU: 3-22                   [64, 16, 3, 3]            --\n",
       "│    │    └─Conv2d: 3-23                 [64, 16, 3, 3]            2,304\n",
       "│    │    └─BatchNorm2d: 3-24            [64, 16, 3, 3]            32\n",
       "│    │    └─ReLU: 3-25                   [64, 16, 3, 3]            --\n",
       "│    └─BasicBlock: 2-5                   [64, 16, 3, 3]            --\n",
       "│    │    └─Conv2d: 3-26                 [64, 16, 3, 3]            2,304\n",
       "│    │    └─BatchNorm2d: 3-27            [64, 16, 3, 3]            32\n",
       "│    │    └─ReLU: 3-28                   [64, 16, 3, 3]            --\n",
       "│    │    └─Conv2d: 3-29                 [64, 16, 3, 3]            2,304\n",
       "│    │    └─BatchNorm2d: 3-30            [64, 16, 3, 3]            32\n",
       "│    │    └─ReLU: 3-31                   [64, 16, 3, 3]            --\n",
       "├─Sequential: 1-7                        [64, 32, 2, 2]            --\n",
       "│    └─BasicBlock: 2-6                   [64, 32, 2, 2]            --\n",
       "│    │    └─Conv2d: 3-32                 [64, 32, 2, 2]            4,608\n",
       "│    │    └─BatchNorm2d: 3-33            [64, 32, 2, 2]            64\n",
       "│    │    └─ReLU: 3-34                   [64, 32, 2, 2]            --\n",
       "│    │    └─Conv2d: 3-35                 [64, 32, 2, 2]            9,216\n",
       "│    │    └─BatchNorm2d: 3-36            [64, 32, 2, 2]            64\n",
       "│    │    └─Sequential: 3-37             [64, 32, 2, 2]            576\n",
       "│    │    └─ReLU: 3-38                   [64, 32, 2, 2]            --\n",
       "│    └─BasicBlock: 2-7                   [64, 32, 2, 2]            --\n",
       "│    │    └─Conv2d: 3-39                 [64, 32, 2, 2]            9,216\n",
       "│    │    └─BatchNorm2d: 3-40            [64, 32, 2, 2]            64\n",
       "│    │    └─ReLU: 3-41                   [64, 32, 2, 2]            --\n",
       "│    │    └─Conv2d: 3-42                 [64, 32, 2, 2]            9,216\n",
       "│    │    └─BatchNorm2d: 3-43            [64, 32, 2, 2]            64\n",
       "│    │    └─ReLU: 3-44                   [64, 32, 2, 2]            --\n",
       "│    └─BasicBlock: 2-8                   [64, 32, 2, 2]            --\n",
       "│    │    └─Conv2d: 3-45                 [64, 32, 2, 2]            9,216\n",
       "│    │    └─BatchNorm2d: 3-46            [64, 32, 2, 2]            64\n",
       "│    │    └─ReLU: 3-47                   [64, 32, 2, 2]            --\n",
       "│    │    └─Conv2d: 3-48                 [64, 32, 2, 2]            9,216\n",
       "│    │    └─BatchNorm2d: 3-49            [64, 32, 2, 2]            64\n",
       "│    │    └─ReLU: 3-50                   [64, 32, 2, 2]            --\n",
       "├─AdaptiveAvgPool2d: 1-8                 [64, 32, 1, 1]            --\n",
       "├─Linear: 1-9                            [64, 10]                  330\n",
       "==========================================================================================\n",
       "Total params: 67,778\n",
       "Trainable params: 67,778\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 29.15\n",
       "==========================================================================================\n",
       "Input size (MB): 0.20\n",
       "Forward/backward pass size (MB): 4.38\n",
       "Params size (MB): 0.27\n",
       "Estimated Total Size (MB): 4.85\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers = [2, 3, 3]\n",
    "model = ResNet(BasicBlock, layers, num_classes=10).to(device)\n",
    "summary(model, input_size=(BATCH_SIZE, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b41e68bf-8ede-4678-a60c-c797059d643e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optmizer\n",
    "learning_rate = 0.001\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e20baa97-2096-46ba-b166-1bfb7dccb1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Epoch 1 of 100\n",
      "Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "079cedb26a1140e29fe31aa4dac4a46e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/844 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc51fb762c1346cdb406d4aa5a8a9b23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "training loss: 1.363, training acc: 62.941%\n",
      "validation loss: 0.753, validation acc: 83.067%\n",
      "--------------------------------------------------\n",
      "[INFO]: Epoch 2 of 100\n",
      "Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dbe8c0d72ad43299b280647459a717d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/844 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/xy/jf917ryj1p1fw50z30zbs57r0000gn/T/ipykernel_1667/2410146746.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[INFO]: Epoch {epoch+1} of {EPOCHS}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain_epoch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_epoch_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mval_epoch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_epoch_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/python_projects/nn_cnn_main/pytorch/train_test.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, trainloader, optimizer, criterion)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# calculate the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/xy/jf917ryj1p1fw50z30zbs57r0000gn/T/ipykernel_1667/793460043.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madaptAvgPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/xy/jf917ryj1p1fw50z30zbs57r0000gn/T/ipykernel_1667/793460043.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0midentity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1_bn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2_bn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# apply the relu after the skipped connection addition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mused\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnormalization\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meval\u001b[0m \u001b[0mmode\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mbuffers\u001b[0m \u001b[0mare\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \"\"\"\n\u001b[0;32m--> 171\u001b[0;31m         return F.batch_norm(\n\u001b[0m\u001b[1;32m    172\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0;31m# If buffers are not to be tracked, ensure that they won't be updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2448\u001b[0m         \u001b[0m_verify_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2450\u001b[0;31m     return torch.batch_norm(\n\u001b[0m\u001b[1;32m   2451\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2452\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"[INFO]: Epoch {epoch+1} of {EPOCHS}\")\n",
    "    train_epoch_loss, train_epoch_acc = train(model, train_loader, optimizer, criterion)\n",
    "    print('\\n')\n",
    "    val_epoch_loss, val_epoch_acc = test(model, val_loader, criterion)\n",
    "    \n",
    "    print('\\n')\n",
    "    print(f\"training loss: {train_epoch_loss:.3f}, training acc: {train_epoch_acc:.3f}%\")\n",
    "    print(f\"validation loss: {val_epoch_loss:.3f}, validation acc: {val_epoch_acc:.3f}%\")\n",
    "    print('-'*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
