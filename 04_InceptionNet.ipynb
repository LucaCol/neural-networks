{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0481242f-52a6-42ff-bbd9-f1d2f67b54b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchinfo import summary\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from math import ceil\n",
    "#from tqdm.notebook import tqdm\n",
    "\n",
    "from train_test_aux import train, test\n",
    "\n",
    "# device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748e5f2f-408e-4390-9f95-98635748692a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17b1a4e5-260c-419d-a72b-58416404d925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "844\n",
      "94\n",
      "157\n"
     ]
    }
   ],
   "source": [
    "VAL_SIZE = 0.1\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# download the training and testing datasets\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data',\n",
    "                                           train=True,\n",
    "                                           transform=transforms.ToTensor(),\n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='./root',\n",
    "                                          train=False,\n",
    "                                          transform=transforms.ToTensor(),\n",
    "                                          download=True)\n",
    "\n",
    "# split the training set into train and validation sets\n",
    "train_indices, val_indices, _, _ = train_test_split(range(len(train_dataset)),\n",
    "                                                    train_dataset.targets,\n",
    "                                                    stratify=train_dataset.targets,\n",
    "                                                    test_size=VAL_SIZE)\n",
    "\n",
    "train_split = Subset(train_dataset, train_indices)\n",
    "val_split = Subset(train_dataset, val_indices)\n",
    "\n",
    "# now create the batches of the train, val, test sets\n",
    "train_loader = DataLoader(train_split, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_split, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "print(len(train_loader))\n",
    "print(len(val_loader))\n",
    "print(len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79869f38-c9bb-4163-b966-975e0bd4a72f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show some samples from test set\n",
    "\n",
    "samples = iter(test_loader)\n",
    "sample_data, sample_targets = next(samples)\n",
    "print((sample_data[0][0]).size())\n",
    "\n",
    "for i in range(6):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    plt.imshow(sample_data[i][0], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b63e578-e803-479d-8296-4686985a7ad9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134e0e9d-a14d-4c86-8313-57522e3fd89d",
   "metadata": {},
   "source": [
    "We build 4 main blocks\n",
    "- Convolutional Block (conv+batchNorm+relu)\n",
    "- Inception Block: each one consists of four parallel blocks\n",
    "    1. Block 1: contain filter with kernel size of 1x1\n",
    "    2. Block 2: contain filter with kernel size of 3x3\n",
    "    3. Block 3: contain filter with kernel size of 5x5\n",
    "    4. Block 4: contain max pooling layer with kernel size of 3x3 followed by filter with kernel size of 1x1\n",
    "- Reduction Inception Block: each one consists of four parallel blocks\n",
    "    1. Block 1: contain filter with kernel size of 1x1\n",
    "    2. Block 2: contain filter with kernel size of 1x1 followed by filter with kernel size of 3x3 (3x3 reduce)\n",
    "    3. Block 3: contain filter with kernel size of 1x1 followed by filter with kernel size of 5x5 (5x5 reduce)\n",
    "    4. Block 4: contain max pooling layer with kernel size of 3x3 followed by filter with kernel size of 1x1\n",
    "- Auxiliary Classifier Block: They are classifier heads we attach to layers before the end of the network. The motivation is to push useful gradients to the lower layers to make them immediately useful and improve the convergence during training by combatting the vanishing gradient problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45271fcd-e106-4f47-b45e-ee11f143276b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional Block\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
    "        self.conv_bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv_bn(self.conv(x)))\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91261f8f-ed62-4cf7-a55b-ed85bbc2d3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_1x1, out_3x3, out_5x5, out_1x1pool):\n",
    "        super(InceptionBlock, self).__init__()\n",
    "        \n",
    "        self.branch1 = ConvBlock(in_channels, out_1x1, kernel_size=1)\n",
    "        \n",
    "        self.branch2 = ConvBlock(in_channels, out_3x3, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.branch3 = ConvBlock(in_channels, out_5x5, kernel_size=5, padding=2)\n",
    "        \n",
    "        self.branch4 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
    "            ConvBlock(in_channels, out_1x1pool, kernel_size=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.cat(\n",
    "            [self.branch1(x), self.branch2(x), self.branch3(x), self.branch4(x)], 1\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "122e1f1a-cf52-419c-8130-8aa7f94e2411",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedInceptionBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_1x1, red_3x3, out_3x3, red_5x5, out_5x5, out_1x1pool):\n",
    "        super(RedInceptionBlock, self).__init__()\n",
    "        \n",
    "        self.branch1 = ConvBlock(in_channels, out_1x1, kernel_size=1)\n",
    "        \n",
    "        self.branch2 = nn.Sequential(\n",
    "            ConvBlock(in_channels, red_3x3, kernel_size=1),\n",
    "            ConvBlock(red_3x3, out_3x3, kernel_size=3, padding=1)\n",
    "        )\n",
    "        \n",
    "        self.branch3 = nn.Sequential(\n",
    "            ConvBlock(in_channels, red_5x5, kernel_size=1),\n",
    "            ConvBlock(red_5x5, out_5x5, kernel_size=5, padding=2)\n",
    "        )\n",
    "        \n",
    "        self.branch4 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
    "            ConvBlock(in_channels, out_1x1pool, kernel_size=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.cat(\n",
    "            [self.branch1(x), self.branch2(x), self.branch3(x), self.branch4(x)], 1\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4722b1d-5323-40f4-9a81-b7e183b63aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionAux(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(InceptionAux, self).__init__()\n",
    "        \n",
    "        self.adaptAvgPool = nn.AdaptiveAvgPool2d(output_size=(4, 4))\n",
    "        self.dropout = nn.Dropout(p=0.7)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.conv = ConvBlock(in_channels, 24, kernel_size=1)\n",
    "        self.fc1 = nn.Linear(24*4*4, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.adaptAvgPool(x)\n",
    "        x = self.conv(x)\n",
    "        x = x.reshape(x.shape[0], -1) # flatten the out in a single vector\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01f1288d-3ba0-46e2-a48d-a56c04fc65cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionNet(nn.Module):\n",
    "    def __init__(self, aux_logits=True, num_classes=10):\n",
    "        super(InceptionNet, self).__init__()\n",
    "        \n",
    "        assert aux_logits == True or aux_logits == False\n",
    "        self.aux_logits = aux_logits\n",
    "        \n",
    "        self.conv1 = ConvBlock(in_channels=1, out_channels=8, kernel_size=7, stride=2, padding=3)\n",
    "        self.conv2 = ConvBlock(in_channels=8, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # In this order: in_channels, out_1x1, out_3x3, out_5x5, out_1x1pool\n",
    "        self.inception3a = InceptionBlock(16, 8, 16, 8, 8)\n",
    "        self.inception3b = InceptionBlock(40, 4, 20, 8, 8)\n",
    "        self.inception3c = InceptionBlock(40, 16, 32, 16, 8)\n",
    "\n",
    "        # In this order: in_channels, out_1x1, red_3x3, out_3x3, red_5x5, out_5x5, out_1x1pool\n",
    "        self.inception4a = RedInceptionBlock(72, 32, 20, 40, 12, 24, 16)\n",
    "        self.inception4b = RedInceptionBlock(112, 24, 22, 44, 14, 28, 16)\n",
    "        self.inception4c = RedInceptionBlock(112, 16, 24, 48, 16, 32, 16)\n",
    "        self.inception4d = RedInceptionBlock(112, 16, 26, 54, 18, 38, 16)\n",
    "        \n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.adaptAvgPool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.fc1 = nn.Linear(124, num_classes)\n",
    "        \n",
    "        if self.aux_logits:\n",
    "            self.aux1 = InceptionAux(112, num_classes)\n",
    "        else:\n",
    "            self.aux1 = None\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.inception3a(x)\n",
    "        x = self.inception3b(x)\n",
    "        x = self.inception3c(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.inception4a(x)\n",
    "        \n",
    "        # Auxiliary Softmax classifier 1\n",
    "        if self.aux_logits and self.training:\n",
    "            aux1 = self.aux1(x)\n",
    "        \n",
    "        x = self.inception4b(x)\n",
    "        x = self.inception4c(x)\n",
    "        x = self.inception4d(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.adaptAvgPool(x)\n",
    "        x = x.reshape(x.shape[0], -1) # flatten the out in a single vector\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        if self.aux_logits and self.training:\n",
    "            return aux1, x\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10e42f8f-f657-4a81-8768-b85742017d3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "InceptionNet                             [64, 10]                  53,306\n",
       "├─ConvBlock: 1-1                         [64, 8, 14, 14]           --\n",
       "│    └─Conv2d: 2-1                       [64, 8, 14, 14]           392\n",
       "│    └─BatchNorm2d: 2-2                  [64, 8, 14, 14]           16\n",
       "│    └─ReLU: 2-3                         [64, 8, 14, 14]           --\n",
       "├─MaxPool2d: 1-2                         [64, 8, 7, 7]             --\n",
       "├─ConvBlock: 1-3                         [64, 16, 7, 7]            --\n",
       "│    └─Conv2d: 2-4                       [64, 16, 7, 7]            1,152\n",
       "│    └─BatchNorm2d: 2-5                  [64, 16, 7, 7]            32\n",
       "│    └─ReLU: 2-6                         [64, 16, 7, 7]            --\n",
       "├─MaxPool2d: 1-4                         [64, 16, 4, 4]            --\n",
       "├─InceptionBlock: 1-5                    [64, 40, 4, 4]            --\n",
       "│    └─ConvBlock: 2-7                    [64, 8, 4, 4]             --\n",
       "│    │    └─Conv2d: 3-1                  [64, 8, 4, 4]             128\n",
       "│    │    └─BatchNorm2d: 3-2             [64, 8, 4, 4]             16\n",
       "│    │    └─ReLU: 3-3                    [64, 8, 4, 4]             --\n",
       "│    └─ConvBlock: 2-8                    [64, 16, 4, 4]            --\n",
       "│    │    └─Conv2d: 3-4                  [64, 16, 4, 4]            2,304\n",
       "│    │    └─BatchNorm2d: 3-5             [64, 16, 4, 4]            32\n",
       "│    │    └─ReLU: 3-6                    [64, 16, 4, 4]            --\n",
       "│    └─ConvBlock: 2-9                    [64, 8, 4, 4]             --\n",
       "│    │    └─Conv2d: 3-7                  [64, 8, 4, 4]             3,200\n",
       "│    │    └─BatchNorm2d: 3-8             [64, 8, 4, 4]             16\n",
       "│    │    └─ReLU: 3-9                    [64, 8, 4, 4]             --\n",
       "│    └─Sequential: 2-10                  [64, 8, 4, 4]             --\n",
       "│    │    └─MaxPool2d: 3-10              [64, 16, 4, 4]            --\n",
       "│    │    └─ConvBlock: 3-11              [64, 8, 4, 4]             144\n",
       "├─InceptionBlock: 1-6                    [64, 40, 4, 4]            --\n",
       "│    └─ConvBlock: 2-11                   [64, 4, 4, 4]             --\n",
       "│    │    └─Conv2d: 3-12                 [64, 4, 4, 4]             160\n",
       "│    │    └─BatchNorm2d: 3-13            [64, 4, 4, 4]             8\n",
       "│    │    └─ReLU: 3-14                   [64, 4, 4, 4]             --\n",
       "│    └─ConvBlock: 2-12                   [64, 20, 4, 4]            --\n",
       "│    │    └─Conv2d: 3-15                 [64, 20, 4, 4]            7,200\n",
       "│    │    └─BatchNorm2d: 3-16            [64, 20, 4, 4]            40\n",
       "│    │    └─ReLU: 3-17                   [64, 20, 4, 4]            --\n",
       "│    └─ConvBlock: 2-13                   [64, 8, 4, 4]             --\n",
       "│    │    └─Conv2d: 3-18                 [64, 8, 4, 4]             8,000\n",
       "│    │    └─BatchNorm2d: 3-19            [64, 8, 4, 4]             16\n",
       "│    │    └─ReLU: 3-20                   [64, 8, 4, 4]             --\n",
       "│    └─Sequential: 2-14                  [64, 8, 4, 4]             --\n",
       "│    │    └─MaxPool2d: 3-21              [64, 40, 4, 4]            --\n",
       "│    │    └─ConvBlock: 3-22              [64, 8, 4, 4]             336\n",
       "├─InceptionBlock: 1-7                    [64, 72, 4, 4]            --\n",
       "│    └─ConvBlock: 2-15                   [64, 16, 4, 4]            --\n",
       "│    │    └─Conv2d: 3-23                 [64, 16, 4, 4]            640\n",
       "│    │    └─BatchNorm2d: 3-24            [64, 16, 4, 4]            32\n",
       "│    │    └─ReLU: 3-25                   [64, 16, 4, 4]            --\n",
       "│    └─ConvBlock: 2-16                   [64, 32, 4, 4]            --\n",
       "│    │    └─Conv2d: 3-26                 [64, 32, 4, 4]            11,520\n",
       "│    │    └─BatchNorm2d: 3-27            [64, 32, 4, 4]            64\n",
       "│    │    └─ReLU: 3-28                   [64, 32, 4, 4]            --\n",
       "│    └─ConvBlock: 2-17                   [64, 16, 4, 4]            --\n",
       "│    │    └─Conv2d: 3-29                 [64, 16, 4, 4]            16,000\n",
       "│    │    └─BatchNorm2d: 3-30            [64, 16, 4, 4]            32\n",
       "│    │    └─ReLU: 3-31                   [64, 16, 4, 4]            --\n",
       "│    └─Sequential: 2-18                  [64, 8, 4, 4]             --\n",
       "│    │    └─MaxPool2d: 3-32              [64, 40, 4, 4]            --\n",
       "│    │    └─ConvBlock: 3-33              [64, 8, 4, 4]             336\n",
       "├─MaxPool2d: 1-8                         [64, 72, 2, 2]            --\n",
       "├─RedInceptionBlock: 1-9                 [64, 112, 2, 2]           --\n",
       "│    └─ConvBlock: 2-19                   [64, 32, 2, 2]            --\n",
       "│    │    └─Conv2d: 3-34                 [64, 32, 2, 2]            2,304\n",
       "│    │    └─BatchNorm2d: 3-35            [64, 32, 2, 2]            64\n",
       "│    │    └─ReLU: 3-36                   [64, 32, 2, 2]            --\n",
       "│    └─Sequential: 2-20                  [64, 40, 2, 2]            --\n",
       "│    │    └─ConvBlock: 3-37              [64, 20, 2, 2]            1,480\n",
       "│    │    └─ConvBlock: 3-38              [64, 40, 2, 2]            7,280\n",
       "│    └─Sequential: 2-21                  [64, 24, 2, 2]            --\n",
       "│    │    └─ConvBlock: 3-39              [64, 12, 2, 2]            888\n",
       "│    │    └─ConvBlock: 3-40              [64, 24, 2, 2]            7,248\n",
       "│    └─Sequential: 2-22                  [64, 16, 2, 2]            --\n",
       "│    │    └─MaxPool2d: 3-41              [64, 72, 2, 2]            --\n",
       "│    │    └─ConvBlock: 3-42              [64, 16, 2, 2]            1,184\n",
       "├─RedInceptionBlock: 1-10                [64, 112, 2, 2]           --\n",
       "│    └─ConvBlock: 2-23                   [64, 24, 2, 2]            --\n",
       "│    │    └─Conv2d: 3-43                 [64, 24, 2, 2]            2,688\n",
       "│    │    └─BatchNorm2d: 3-44            [64, 24, 2, 2]            48\n",
       "│    │    └─ReLU: 3-45                   [64, 24, 2, 2]            --\n",
       "│    └─Sequential: 2-24                  [64, 44, 2, 2]            --\n",
       "│    │    └─ConvBlock: 3-46              [64, 22, 2, 2]            2,508\n",
       "│    │    └─ConvBlock: 3-47              [64, 44, 2, 2]            8,800\n",
       "│    └─Sequential: 2-25                  [64, 28, 2, 2]            --\n",
       "│    │    └─ConvBlock: 3-48              [64, 14, 2, 2]            1,596\n",
       "│    │    └─ConvBlock: 3-49              [64, 28, 2, 2]            9,856\n",
       "│    └─Sequential: 2-26                  [64, 16, 2, 2]            --\n",
       "│    │    └─MaxPool2d: 3-50              [64, 112, 2, 2]           --\n",
       "│    │    └─ConvBlock: 3-51              [64, 16, 2, 2]            1,824\n",
       "├─RedInceptionBlock: 1-11                [64, 112, 2, 2]           --\n",
       "│    └─ConvBlock: 2-27                   [64, 16, 2, 2]            --\n",
       "│    │    └─Conv2d: 3-52                 [64, 16, 2, 2]            1,792\n",
       "│    │    └─BatchNorm2d: 3-53            [64, 16, 2, 2]            32\n",
       "│    │    └─ReLU: 3-54                   [64, 16, 2, 2]            --\n",
       "│    └─Sequential: 2-28                  [64, 48, 2, 2]            --\n",
       "│    │    └─ConvBlock: 3-55              [64, 24, 2, 2]            2,736\n",
       "│    │    └─ConvBlock: 3-56              [64, 48, 2, 2]            10,464\n",
       "│    └─Sequential: 2-29                  [64, 32, 2, 2]            --\n",
       "│    │    └─ConvBlock: 3-57              [64, 16, 2, 2]            1,824\n",
       "│    │    └─ConvBlock: 3-58              [64, 32, 2, 2]            12,864\n",
       "│    └─Sequential: 2-30                  [64, 16, 2, 2]            --\n",
       "│    │    └─MaxPool2d: 3-59              [64, 112, 2, 2]           --\n",
       "│    │    └─ConvBlock: 3-60              [64, 16, 2, 2]            1,824\n",
       "├─RedInceptionBlock: 1-12                [64, 124, 2, 2]           --\n",
       "│    └─ConvBlock: 2-31                   [64, 16, 2, 2]            --\n",
       "│    │    └─Conv2d: 3-61                 [64, 16, 2, 2]            1,792\n",
       "│    │    └─BatchNorm2d: 3-62            [64, 16, 2, 2]            32\n",
       "│    │    └─ReLU: 3-63                   [64, 16, 2, 2]            --\n",
       "│    └─Sequential: 2-32                  [64, 54, 2, 2]            --\n",
       "│    │    └─ConvBlock: 3-64              [64, 26, 2, 2]            2,964\n",
       "│    │    └─ConvBlock: 3-65              [64, 54, 2, 2]            12,744\n",
       "│    └─Sequential: 2-33                  [64, 38, 2, 2]            --\n",
       "│    │    └─ConvBlock: 3-66              [64, 18, 2, 2]            2,052\n",
       "│    │    └─ConvBlock: 3-67              [64, 38, 2, 2]            17,176\n",
       "│    └─Sequential: 2-34                  [64, 16, 2, 2]            --\n",
       "│    │    └─MaxPool2d: 3-68              [64, 112, 2, 2]           --\n",
       "│    │    └─ConvBlock: 3-69              [64, 16, 2, 2]            1,824\n",
       "├─MaxPool2d: 1-13                        [64, 124, 1, 1]           --\n",
       "├─AdaptiveAvgPool2d: 1-14                [64, 124, 1, 1]           --\n",
       "├─Dropout: 1-15                          [64, 124]                 --\n",
       "├─Linear: 1-16                           [64, 10]                  1,250\n",
       "==========================================================================================\n",
       "Total params: 224,260\n",
       "Trainable params: 224,260\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 89.69\n",
       "==========================================================================================\n",
       "Input size (MB): 0.20\n",
       "Forward/backward pass size (MB): 7.41\n",
       "Params size (MB): 0.68\n",
       "Estimated Total Size (MB): 8.30\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = InceptionNet(aux_logits=True, num_classes=10)\n",
    "summary(model, input_size=(BATCH_SIZE, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25c09e1f-f203-4705-b545-bf9210352d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optmizer\n",
    "learning_rate = 0.001\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b0cecec-717d-461f-a51a-279a3fc8b230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Epoch 1 of 100\n",
      "Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a2020c941084c85828b8b087e5eb041",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/844 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84f6777884224b54878052e1bc2de4b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "training loss: 1.919, training acc: 62.485%\n",
      "validation loss: 0.522, validation acc: 88.233%\n",
      "--------------------------------------------------\n",
      "[INFO]: Epoch 2 of 100\n",
      "Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d80c700991d459ea36a81fcc8f4428e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/844 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba048cdec89f4dc98cc95d1893e6bc1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "training loss: 0.945, training acc: 90.144%\n",
      "validation loss: 0.246, validation acc: 93.583%\n",
      "--------------------------------------------------\n",
      "[INFO]: Epoch 3 of 100\n",
      "Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34caa92190e0499c8e4f7d5ea8dad6f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/844 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59552893ed7d461a9bd47766e6818754",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "training loss: 0.686, training acc: 93.456%\n",
      "validation loss: 0.171, validation acc: 95.200%\n",
      "--------------------------------------------------\n",
      "[INFO]: Epoch 4 of 100\n",
      "Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6fc5cbb49b14db6aebce94578823939",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/844 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/xy/jf917ryj1p1fw50z30zbs57r0000gn/T/ipykernel_1106/2410146746.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[INFO]: Epoch {epoch+1} of {EPOCHS}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain_epoch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_epoch_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mval_epoch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_epoch_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/python_projects/nn_cnn_main/pytorch/train_test_aux.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, trainloader, optimizer, criterion)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0maux1_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# calculate the loss for aux1 and the entire model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/xy/jf917ryj1p1fw50z30zbs57r0000gn/T/ipykernel_1106/4052512050.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minception4a\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m# Auxiliary Softmax classifier 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/xy/jf917ryj1p1fw50z30zbs57r0000gn/T/ipykernel_1106/3782922853.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         return torch.cat(\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbranch1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbranch2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbranch3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbranch4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/xy/jf917ryj1p1fw50z30zbs57r0000gn/T/ipykernel_1106/3810267037.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_bn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 459\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"[INFO]: Epoch {epoch+1} of {EPOCHS}\")\n",
    "    train_epoch_loss, train_epoch_acc = train(model, train_loader, optimizer, criterion)\n",
    "    print('\\n')\n",
    "    val_epoch_loss, val_epoch_acc = test(model, val_loader, criterion)\n",
    "    \n",
    "    print('\\n')\n",
    "    print(f\"training loss: {train_epoch_loss:.3f}, training acc: {train_epoch_acc:.3f}%\")\n",
    "    print(f\"validation loss: {val_epoch_loss:.3f}, validation acc: {val_epoch_acc:.3f}%\")\n",
    "    print('-'*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
